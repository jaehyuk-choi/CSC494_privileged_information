# Leveraging LLMs to Generate Privileged Information for Clinical Predictive Tasks

This repository accompanies the research project **"Leveraging LLMs to Generate Privileged Information for Clinical Predictive Tasks"** by **David Pellow and Jaehyuk Choi**. The project explores how privileged information generated by large language models (LLMs) can enhance predictive modeling in healthcare by generating synthetic, context-rich auxiliary data used only during training.

---

## ğŸ“Œ Overview

Predictive modeling in clinical settings is often limited by small, imbalanced datasets. Rather than using LLMs as direct predictors, this project investigates their utility in generating **privileged information** â€” synthetic features used exclusively at training time to guide the model.

---

## âš™ï¸ Methodology

1. **Synthetic Data Generation**  
   - LLMs (LLaMA-8B, LLaMA-70B) are prompted to simulate clinical reasoning and generate side information (e.g., HbA1c predictions, risk scores).

2. **Pattern-Based Integration**  
   LLM-generated data is integrated through four training paradigms:

   - **Direct**:  
     LLM-generated scalar features (e.g., predicted HbA1c) are directly appended to the input feature vector during training only.

   - **Multi-task**:  
     The model is jointly trained on the main task and auxiliary LLM-generated labels (e.g., `health_1_10`, `risk_score`, `has_diabetes`) with a shared encoder.

   - **Multi-view**:  
     Original and LLM-generated features are processed via separate encoders (view1/view2). Only view1 is available during testing.

   - **Pairwise Similarity**:  
     The LLM generates similarity scores between patient pairs to supervise models with pairwise inputs.

   - *Hybrid strategies* (e.g., Direct + Multi-task) are also implemented.

3. **Training & Evaluation**  
   Multiple strategies are used:
   - Simultaneous training  
   - Decoupled training  
   - Pretraining followed by fine-tuning

---

## ğŸ§ª Datasets

- **UCI Diabetes Dataset**  
  Predicts diabetes onset based on demographic and lifestyle features.

- **MIMIC-IV Dataset**  
  Predicts in-hospital mortality using ICU patient time-series data.

---

## ğŸ“ Directory Structure & Usage

```bash
CSC494_PRIVILEGED_INFORMATION/
â”œâ”€â”€ data/                # Preprocessing scripts for each pattern
â”œâ”€â”€ model/               # Model definitions for each integration pattern
â”œâ”€â”€ prompting/           # LLM-generated side info & similarity data
â”œâ”€â”€ img/                 # Visualizations (Train & Validation Loss)
â”œâ”€â”€ utils.py             # Shared functions (grid search, experiment runner)
â”œâ”€â”€ direct.py            # Train using Direct pattern
â”œâ”€â”€ multitask.py         # Train using Multi-task pattern
â”œâ”€â”€ multiview.py         # Train using Multi-view pattern
â”œâ”€â”€ pairwise.py          # Train using Pairwise pattern
â”œâ”€â”€ explore_data.py      # EDA and dataset summary
â””â”€â”€ *.csv                # Evaluation results
```

---
## ğŸš€ Getting Started

Install dependencies:

```bash
pip install -r requirements.txt
```

## âš™ï¸ File Roles & Execution Guide

```text
data/
    Preprocessing modules for each learning pattern:
        - baseline_data.py
        - direct_data.py
        - multitask_data.py
        - multiview_data.py
        - pairwise_data.py

model/
    Architecture implementations by pattern:
        - Direct (with/without residuals or decompression)
        - Multi-task with attention-based heads
        - Multi-view dual-encoder models
        - Pairwise MLP similarity models

utils.py
    Shared functionality:
        - grid_search: hyperparameter tuning over validation set
        - run_experiments: runs 10 randomized training iterations for evaluation
```
